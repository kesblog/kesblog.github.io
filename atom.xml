<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://kesblog.github.io</id>
    <title>KesBlog</title>
    <updated>2019-08-26T14:07:37.487Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://kesblog.github.io"/>
    <link rel="self" href="https://kesblog.github.io/atom.xml"/>
    <subtitle>Every 🐦 has an 🦅&apos;s dream.</subtitle>
    <logo>https://kesblog.github.io/images/avatar.png</logo>
    <icon>https://kesblog.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, KesBlog</rights>
    <entry>
        <title type="html"><![CDATA[How to Fine-Tune BERT for Text Classification]]></title>
        <id>https://kesblog.github.io/post/how-to-fine-tune-bert-for-text-classification</id>
        <link href="https://kesblog.github.io/post/how-to-fine-tune-bert-for-text-classification">
        </link>
        <updated>2019-08-26T10:26:16.000Z</updated>
        <summary type="html"><![CDATA[<p>预训练语言模型在学习通用的语言表示方面被证明是有用的。作为SOTA的预训练语言模型，BERT在许多语言理解任务中取得了很好的许熬过。在这篇文章中，我们进行了详细的实验来探索不同的BERT在文本分类上的fine-tuning方法，并且提出了一个BERT fine-tuning的通用方案。最终，提出的新方案在8个广泛研究的文本分类数据机上取得了SOTA的结果。</p>
]]></summary>
        <content type="html"><![CDATA[<p>预训练语言模型在学习通用的语言表示方面被证明是有用的。作为SOTA的预训练语言模型，BERT在许多语言理解任务中取得了很好的许熬过。在这篇文章中，我们进行了详细的实验来探索不同的BERT在文本分类上的fine-tuning方法，并且提出了一个BERT fine-tuning的通用方案。最终，提出的新方案在8个广泛研究的文本分类数据机上取得了SOTA的结果。</p>
<!-- more -->
<h2 id="介绍">介绍</h2>
<p>文本分类是NLP中的经典任务，之前的做法有CNN、RNN和Attention mechanisms。</p>
<p>目前大量的研究表明，在大规模预料上训练的模型，可以避免从头开始训练一个新的模型，对于文本分类以及其他的NLP任务是有帮助的。一种预训练模式是word embeddings，比如word2vec、GloVe，或者是contextualized word embeddings，比如Cove和ELMo。另一种预训练模型是橘子级别的，Howard和Ruder提出了ULMFiT，一种对预训练模型的fine-tuning方法，在6个广泛研究的文本分类数据集中取得了SOTA的结果。最近，预训练模型被证明是有用的在学习common language representations方面，通过利用大规模未标注的预料。例如Open AI的GPT和BERT。BERT基于多层双向Transformer，在masked word prediction和next sentence prediction tasks使用plain text进行训练。</p>
<p>虽然BERT在许多自然语言理解任务中取得了惊人的成绩，它的潜力还没有被完整的发掘。目前几乎没有对增强BERT在目标任务表现取得更进一步的研究。</p>
<p>在这篇文中中，我们探索了如何在文本分类任务中最大化的利用BERT。我们探索了一些对BERT进行fine-tuning的方法来增强其在文本人类任务中的表现，我们设计了详尽的实验来对BERT进行详细的分析。</p>
<p>这篇论文的贡献如下：</p>
<ul>
<li>我们提出了一个通用的对BERT进行fine-tuning的方案，包含三个步骤：（1）继续在within-task training data或者in-domain data上预训练BERT。（2）可选的使用多任务学习来fine-tuning BERT如果存在一些可行的相关任务。（3）在目标任务上进行fine-tuning</li>
<li>我们也探索了不同的BERT在目标任务上的fine-tuning方法，包括对长文本进行预处理，层选择，层尺度的学习率，灾难性遗忘问题（catastrophic foggetting）和小样本学习问题（low-shot learning problems）</li>
<li>我们在7个广泛研究的英文文本分类数据集和1个中文新闻类别数据集中取得了SOTA的结果。</li>
</ul>
<h2 id="相关工作">相关工作</h2>
<p>借鉴从其他任务学习的知识在NLP领域越来越热门。这里主要回顾两个相关的方法：<strong>预训练语言模型</strong>和<strong>多任务学习</strong></p>
<h3 id="预训练语言模型">预训练语言模型</h3>
<p>预训练此响亮，最为一个现代NLP系统的重要组成部分，相比从头进行训练，可以带来大量的提升。作为词向量的拓展，句向量、段向量，也在下游的任务中被作为特称而广泛使用。</p>
<p>最近，在大的网络上面使用大规模的未标注数据进行预训练，然后在下游的任务上进行fine-tuning已经在一些自然语言理解的任务上取得了突破，比如GPT和BERT。</p>
<p>在本文中，我们将进一步探索BERT在文本分类任务上的fine-tuning方法。</p>
<h3 id="多任务学习">多任务学习</h3>
<p>多任务学习是另一个相关的方向。Rei（2017）和Liu et al.（2018）使用多任务学习的方法来联合的训练住任务和语言模型。Liu et al.(2019)扩展了MT-DNN模型通过加入BERT作为它的共享的text encoding层。MTL要求训练任务每次从头开始，非常的低效并且要求仔细的对任务进行调参。然而，使用多任务学习将BERT进行fine-tuning可以通过使用共享的预训练模型来避免这个问题。</p>
<h2 id="使用bert进行文本分类">使用BERT进行文本分类</h2>
<p>BERT的输入不超过512 tokens，输出句子的表示。输入的句子有一个活着两个部分，第一个token是[CLS]标签，是整个句子的表示，还有一个[SEP]标签，用来分隔句子。</p>
<p>对于文本分类任务，BERT使用最后一层的[CLS] token的隐藏层来作为整个句子的代表，通过使用softmax来预测句子属于类别 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">c</span></span></span></span> 的概率</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable side="right"><mlabeledtr><mtd><mtext>(1)</mtext></mtd><mtd><mrow><mi>p</mi><mo>(</mo><mi>c</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">h</mi><mo>)</mo><mo>=</mo><mi mathvariant="normal">softmax</mi><mo>⁡</mo><mo>(</mo><mi>W</mi><mi mathvariant="bold">h</mi><mo>)</mo></mrow></mtd></mlabeledtr></mtable><annotation encoding="application/x-tex">p(c | \mathbf{h})=\operatorname{softmax}(W \mathbf{h}) \tag{1}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mord">∣</span><span class="mord"><span class="mord mathbf">h</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop"><span class="mord mathrm">s</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord"><span class="mord mathbf">h</span></span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span> 是任务特定的参数矩阵。我们通过最大化正确标签的log-probability来同时fine-tuning所有的BERT参数和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span></p>
<h2 id="方法">方法</h2>
<p>在这篇文章中，我们使用一下三种方法来寻找合适的fine-tuning方法。</p>
<ol>
<li><strong>Fine-Tuning 策略</strong>：当我们fine-tune BERT的时候，有许多方法来利用BERT。比如，BERT不同的层捕获到不同级别的语法语义方面的信息，哪一层对目标任务最有效？怎样选择更好的优化算法和学习率？</li>
<li><strong>进一步预训练</strong>：BERT是在通用领域被预训练的，和目标领域的数据有着不同的分布。一个自然的想法是在目标领域进一步预训练BERT。</li>
<li><strong>多任务FineTuning</strong>：在不使用预训练模型的时候，多任务学习在使用不同任务之间的共享知识方面有着效果。当在同一目标领域有着多个任务的时候，同时在这多个任务上fine-tune BERT是否仍然有效？</li>
</ol>
<p>fine-tuning BERT的通用方法见图1。<br>
<img src="https://kesblog.github.io/post-images/1566821412795.png" alt=""></p>
<h3 id="fine-tuning-策略">Fine-Tuning 策略</h3>
<p>神经网络不同的层可以捕获不同级别的语法语义的信息。</p>
<p>为了将BERT使用在特定任务上，我们需要考虑一下因素：</p>
<ol>
<li>对 <strong>长文本</strong> 的预处理，因为BERT的最大输入是512</li>
<li>层次选择。官方的BERT-base模型包含了一个embedding layer，12层的encoder和一个pooling layer。我们需要选择对于文本分类任务最有效的层。</li>
<li>过拟合问题。需要一个好的带有合适学习率optimizer。</li>
</ol>
<p>从直觉上来说，BERT模型的低层可能会包含更通用的信息，我们可以对于不同的层使用不同的学习率来进行fine-tuning。</p>
<p>根据Howard和Ruder(2018)，我们将参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span> 分成 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>{</mo><msup><mi>θ</mi><mn>1</mn></msup><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>θ</mi><mi>L</mi></msup><mo>}</mo></mrow><annotation encoding="application/x-tex">\{\theta^1,…,\theta^L\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">L</span></span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> ，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">\theta^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span> 包含着BERT第 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 层的参数。然后参数按照以下方式进行更新。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable side="right"><mlabeledtr><mtd><mtext>(2)</mtext></mtd><mtd><mrow><msubsup><mi>θ</mi><mi>t</mi><mi>l</mi></msubsup><mo>=</mo><msubsup><mi>θ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>l</mi></msubsup><mo>−</mo><msup><mi>η</mi><mi>l</mi></msup><mo>⋅</mo><msub><mi mathvariant="normal">∇</mi><msup><mi>θ</mi><mi>l</mi></msup></msub><mi>J</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow></mtd></mlabeledtr></mtable><annotation encoding="application/x-tex">\theta_{t}^{l}=\theta_{t-1}^{l}-\eta^{l} \cdot \nabla_{\theta^{l}} J(\theta) \tag{2}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.146108em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.204439em;vertical-align:-0.305331em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.899108em;"><span style="top:-2.4530000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.305331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.093548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.49738em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7820285714285713em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2026199999999999em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:1.204439em;vertical-align:-0.305331em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>η</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">\eta^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span> 代表第 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 层的学习率。</p>
<p>我们设置基本的学习率为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>η</mi><mi>L</mi></msup></mrow><annotation encoding="application/x-tex">\eta^L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.035771em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">L</span></span></span></span></span></span></span></span></span></span></span> ，使用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>η</mi><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mi>ξ</mi><mo>⋅</mo><msup><mi>η</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">\eta^{k-1}=\xi \cdot \eta^{k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.04601em;">ξ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span></span> ，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ξ</mi></mrow><annotation encoding="application/x-tex">\xi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.04601em;">ξ</span></span></span></span> 是衰退因子，小于或者等于1。当 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ξ</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\xi &lt; 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.04601em;">ξ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> ，低层有着比高层更小的学习率，当 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ξ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\xi = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.04601em;">ξ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> ，所有的层次有着相同的学习率，此时等同于SGD算法。我们将会在5.3节中探索这些因素。</p>
<h3 id="进一步预训练">进一步预训练</h3>
<p>BERT是在通用领域的预料上进行训练的。对于特定领域的文本分类任务来说，比如电影评论，它的数据分布和BERT的非常不同。因此，我们可以使用masked language model和next sentence prediction任务在相关领域数据上对BERT进一步预训练。采用了三种进一步预训练的方法：</p>
<ol>
<li><strong>Within-task pre-training</strong>：BERT继续在目标任务的训练集上进一步预训练。</li>
<li><strong>In-domain pre-training</strong>：预训练数据来自目标任务的相同领域。比如有不同的情感分类任务，它们有着类似的数据分布，我们可以将这些数据集混合在一起之后对BERT进行进一步的预训练。</li>
<li><strong>Cross-domain pre-training</strong>：预训练数据来自相同和不同领域</li>
</ol>
<p>我们将会在5.4节研究这些方法。</p>
<h3 id="多任务fine-tuning">多任务Fine-Tuning</h3>
<p>多任务学习是一个有效的方法来分享从不同的监督任务中获得的知识。我们在多任务学习框架中fine-tuning BERT进行文本分类。</p>
<p>所有的任务共享BERT层和embedding层。唯一不共享的层是最后的分类层，意味着每个任务有着自己的分类层。实验分析见5.5节。</p>
<h2 id="实验">实验</h2>
<p>我们探索了对7个英文和1个中文文本分类数据集不同的fine-tuning方法。我们使用uncased BERT-base 模型和中文BERT-base 模型。</p>
<h3 id="数据集">数据集</h3>
<p>我们在8个广泛研究的数据集上测试我们的方法。这些数据集有着不同数量的文档和不同长度的文档，覆盖了三个通常的文本分类任务：情感分类，问题分类和话题分类。这些数据集的信息见表1。<br>
<img src="https://kesblog.github.io/post-images/1566823356633.png" alt=""></p>
<ul>
<li><strong>情感分类</strong>：对于情感分析，我们使用了二元电影评论IMDb dataset 和二分类及五分类的Yelp review dataset。</li>
<li><strong>问题分类</strong>：对于问题分类，我们在六分类的TREC dataset和 Yahoo！Answers dataset上测试我们的方法。TREC dataset是开放领域基于事实的文本分类数据集。相对于其他的文档级别的数据集来说，TREC是句子级别的，样本较少。Yahoo! Answers dataset是一个大的数据集，有着1,400K 的训练样本。</li>
<li><strong>话题分类</strong>：对于话题分类，使用了大规模的AG's News 和DBPedia数据集。为了测试BERT的中文效果，我们建立了一个中文基于搜狗新闻预料的训练和测试数据集。我们直接使用中文字符而不是拼音。数据由SogouCA 和 SogouCS 新闻整合。我们使用URL来将新闻进行分类，一共有6个类别，体育、房屋、商业、娱乐、女人和科技。每个类别的训练样本为9000，测试样本为1000。</li>
<li><strong>数据预处理</strong>：使用WordPiece embeddings with 30,000 tokens vocabulary，将分开的word使用 ## 来标记。所以文档的长度数据基于word pieces。对于进一步预训练BERT，我们使用 spaCy来对英文数据集进行句子分割，使用“。”，“？”和“！”来对中文数据进行句子分割。</li>
</ul>
<h3 id="超参数">超参数</h3>
<p>使用BERT-base模型，包含768隐藏单元，12层Transformer和12个self-attention heads。在1块TITAN Xp上对BERT进一步预训练，batch size为32，max sequence length是128，学习率为5e-5，train steps是100,000，warm-up steps是10,000。</p>
<p>在4块TITAN Xp上对BERT进行fine-tuning，batch size为24，dropout为0.1，使用Adam，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">\beta_1=0.9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding="application/x-tex">\beta_2=0.999</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">9</span><span class="mord">9</span></span></span></span>，使用slanted triangular learning rates(ULMFit)，基学习率为2e-5，warm up portion是0.1。我们经验性的设置epoch为4，将验证集上最好的模型保存进行测试。</p>
<h3 id="实验-1探索不同fine-tuning策略">实验-1：探索不同Fine-Tuning策略</h3>
<p>使用IMDb dataset来探索不同的fine-tuning策略。官方的预训练模型为出事的encoder。</p>
<h4 id="处理长文本">处理长文本</h4>
<p>BERT的最大输入长度为512，使用一下三种方式来处理长文本。</p>
<p>#####截断方法</p>
<ol>
<li><strong>head-only</strong>：保留前510个tokens</li>
<li><strong>tail-only</strong>：保留最后510ge tokens</li>
<li><strong>head+tail</strong>：经验性的选择前128和后382个tokens</li>
</ol>
<h5 id="层次方法">层次方法</h5>
<p>首先将输入文本分成 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi><mo>=</mo><mi>L</mi><mi mathvariant="normal">/</mi><mn>510</mn></mrow><annotation encoding="application/x-tex">k=L/510</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">L</span><span class="mord">/</span><span class="mord">5</span><span class="mord">1</span><span class="mord">0</span></span></span></span> 个部分，每部分输入BERT得到k个表示，然后将每个表示最后一层的[CLS]对应的隐藏状态，使用mean pooling，max pooling 和 self-attention来将所有部分的表示组合起来。</p>
<p>表2展示了不同方法的效果。截断方法的<strong>head+tail</strong>效果最好。因此，后续我们将使用该方法处理长文本。<br>
<img src="https://kesblog.github.io/post-images/1566825039114.png" alt=""></p>
<h4 id="不同层的特征">不同层的特征</h4>
<p>不同的层提取输入文本的不同特征，我们探索不同层次特征的效果。然后我们fine-tune模型记录错误率。</p>
<p>表3展示了fine-tuning BERT不同层的效果。BERT的最后一层特征效果最好。因为我们使用该设置来进行后续的实验。<br>
<img src="https://kesblog.github.io/post-images/1566825205263.png" alt=""></p>
<h4 id="灾难性遗忘">灾难性遗忘</h4>
<p>灾难性遗忘是迁移学习中常见的问题，意味着预先学习的知识在学习新知识的过程中被擦除。我们探索BERT是否受该问题的影响。</p>
<p>我们fine-tune BERT使用不同的学习率，图2是错误率的学习曲线。<br>
<img src="https://kesblog.github.io/post-images/1566825344399.png" alt=""></p>
<p>我们发现在较低的学习率，比如2e-5，对于使BERT克服灾难性遗忘是必须的。随着学习率增长到4e-4，训练收敛失败。</p>
<h4 id="层次间学习率衰退">层次间学习率衰退</h4>
<p>表4展示了不同的基学习率和衰退因子的影响。我们发现设置一个较低的学习率在低层上能有效的fine-tune BERT。一个合适的设置是 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ξ</mi><mo>=</mo><mn>0.95</mn><mi mathvariant="normal">，</mi><mi>l</mi><mi>r</mi><mo>=</mo><mn>2.0</mn><mi>e</mi><mo>−</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">\xi=0.95，lr=2.0e-5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.04601em;">ξ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">5</span><span class="mord cjk_fallback">，</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord">.</span><span class="mord">0</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span></span></span></span>。<br>
<img src="https://kesblog.github.io/post-images/1566825578280.png" alt=""></p>
<p>###实验-2：探索进一步预训练</p>
<p>在本节探索进一步预训练。在以下实验的fine-tuning中，使用实验1中的最佳策略。</p>
<h4 id="within-task-进一步预训练">Within-Task 进一步预训练</h4>
<p>我们使用采用不同步骤进一步预训练的模型在目标任务上进一步fine-tuning。</p>
<p>如图3所示，进一步预训练对于提升BERT在特定任务上的表现是有帮助的。<br>
<img src="https://kesblog.github.io/post-images/1566825824830.png" alt=""></p>
<h4 id="in-domain和cross-domain-进一步预训练">In-Domain和Cross-Domain 进一步预训练</h4>
<p>除了在目标任务的训练集上预训练BERT，我们可以在同一领域的数据上进一步预训练BERT。</p>
<p>将7个英文数据集划分为3个领域：<strong>话题</strong>、<strong>情感</strong>和<strong>问题</strong>。划分不是严格正确。</p>
<p>结果在表5中展示。我们发现几乎所有的预训练模型比原始的模型表现的更好。通常来说，in-domain可以带来比within-task更好的表现。在小的句子级别的TREC数据集上，within-task预训练对于表现是有害的，然而in-domain预训练利用了Yah. A. 数据集，因此取得了更好的效果。</p>
<p>cross-domain预训练没有带来显著的提升。<strong>可能是因为BERT已经在通用领域训练过了。</strong></p>
<p>我们也发现IMDb和Yelp在情感方面对对方没有帮助，可能是因为电影和食物的评价数据分布有所差异。<br>
<img src="https://kesblog.github.io/post-images/1566826244139.png" alt=""></p>
<h4 id="同之前的模型进行比较">同之前的模型进行比较</h4>
<p>将我们的模型与之前的模型进行对比，有CNN、RNN、HAN以及ULMFiT等等。</p>
<p>我们使用BERT作为特征进行输入，使用双向带self-attentionLSTM作为输出实现了BERT-Feat模型。BERT-IDPT-Fit对应着表5中的 <em>all sentiment</em>、 <em>all question</em>、<em>all topic</em>，BERT-CDPT-FiT对应着表5的 <em>all</em> 。</p>
<p>如表6所示，BERT-Feat比除了ULMFiT的其他baseline表现的好。除了比BERT-Feat在DBPedia数据集上表现的差，BERT-Fit 比 BERT-Feat 在其他7个数据集上表现的都好。所有的三个进一步预训练的数据集表现的比BERT-FiT模型更好。<br>
<img src="https://kesblog.github.io/post-images/1566826721253.png" alt=""></p>
<h3 id="实验-3多任务fine-tuning">实验-3：多任务Fine-Tuning</h3>
<p>使用了4个英文数据集（IMDb，Yelp P.，AG，DBP）.</p>
<p>使用官方的uncased BERT-base参数，进一步在所有的7个英文数据集上进行预训练。为了得到更好的分类效果，在一起fine-tuning之后，使用小的学习率在各自的数据集上进一步fine-tune。</p>
<p>表7展示了多任务fine-tuning的效果是提升的。但是多任务没有表现出帮助在BERT-CDPT在Yelp P和AG上。多任务学习和cross-domain可能是可以采用的方法，since BERT-CDPT模型已经包含了丰富的领域相关的信息，多任务可能不是那么重要。<br>
<img src="https://kesblog.github.io/post-images/1566827111901.png" alt=""></p>
<h3 id="实验-4小样本学习">实验-4：小样本学习</h3>
<p>预训练模型的一个优点是可以在下游任务上使用小规模的数据。我们测试了BERT-FiT和BERT-ITPT-FiT在不同数量训练集上的效果。我们选择IMDb的子集。结果如图4。<br>
<img src="https://kesblog.github.io/post-images/1566827238533.png" alt=""></p>
<p>实验结果说明BERT对小数据集带来了显著的提升。进一步预训练的BERT可以进一步提升效果。</p>
<h3 id="实验-5在bert-large上进一步预训练">实验-5：在BERT-Large上进一步预训练</h3>
<p>在本节，我们探索BERT-Large是否与BERT-base有相似的效果。我们进一步预训练BERT-Large模型。使用了Tesla-V 100-PCIE 32G GPU，batch size为24，max sequence length为128，120k training steps。对于特定任务的fine-tuning，我们设置batch size为24，在4块Tesla-V 100-PCIE 32G GPUs上，使用max sequence length为512进行fine-tuning。</p>
<p>如表8所示，ULMFiT与BERT-base相比，在所有的任务上几乎表现的更好，但是不如BERT-Large。当进一步预训练加入后，BERT-base在所有的任务上超过了ULMFiT，BERT-Large进一步预训练之后fine-tuning取得了SOTA的结果。<br>
<img src="https://kesblog.github.io/post-images/1566827667443.png" alt=""></p>
<h2 id="结论">结论</h2>
<p>我们探索了BERT在文本分类任务上的不同的fine-tuning方法。有以下发现：</p>
<ol>
<li>BERT的最顶层对于文本分类任务是最有用的。</li>
<li>采用合适的层次间降低的学习率，BERT可以克服灾难性遗忘问题。</li>
<li>Within-task和in-domain进一步预训练可以显著提升分类效果。</li>
<li>多任务fine-tuning对于单任务的fine-tuning是有帮助的，但是效果没有进一步预训练好。</li>
<li>BERT可以小规模数据集任务的性能。</li>
</ol>
<p>通过以上的发现，我们在8个广泛研究的文本分类数据集上取得了SOTA的成绩。未来，我们将会在BERT是如何工作的方面进行深入探索。</p>
]]></content>
    </entry>
</feed>